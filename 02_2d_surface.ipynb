{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from kgi import apply_kgi_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b436357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to enable LaTeX rendering\n",
    "# change to `False` if you get an error during plotting (latex not installed)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_init_to_he_uniform(model):\n",
    "    \"\"\" Change PyTorch's default initialization to He uniform \"\"\"\n",
    "    # PyTorch default uses 1/sqrt(m) as the bound\n",
    "    # He uniform uses sqrt(3)/sqrt(m) as the bound\n",
    "    sqrt3 = np.sqrt(3)\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            layer.weight.data *= sqrt3\n",
    "            layer.bias.data *= sqrt3\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" MLP model \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_hidden_layers=1, activation=torch.relu):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden_layers)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        self.act = activation\n",
    "        default_init_to_he_uniform(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.act(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = layer(x)\n",
    "            x = self.act(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dec2dc",
   "metadata": {},
   "source": [
    "# Visualize initialized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c21ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knots(y, dx, threshold=1e-5):\n",
    "    \"\"\" Find indices of knots on a surface \"\"\"\n",
    "    # neighbours\n",
    "    z_me = y[1:-1, 1:-1]\n",
    "    z_vp = y[2:, 1:-1]\n",
    "    z_vm = y[:-2, 1:-1]\n",
    "    z_hp = y[1:-1, 2:]\n",
    "    z_hm = y[1:-1, :-2]\n",
    "    x_me = torch.zeros_like(z_me)\n",
    "    y_me = torch.zeros_like(z_me)\n",
    "    x_vp = x_me\n",
    "    y_vp = y_me + dx\n",
    "    x_vm = x_me\n",
    "    y_vm = y_me - dx\n",
    "    x_hp = x_me + dx\n",
    "    y_hp = y_me\n",
    "    x_hm = x_me - dx\n",
    "    y_hm = y_me\n",
    "    xyz_me = torch.stack([x_me, y_me, z_me], dim=-1)\n",
    "    xyz_vp = torch.stack([x_vp, y_vp, z_vp], dim=-1)\n",
    "    xyz_vm = torch.stack([x_vm, y_vm, z_vm], dim=-1)\n",
    "    xyz_hp = torch.stack([x_hp, y_hp, z_hp], dim=-1)\n",
    "    xyz_hm = torch.stack([x_hm, y_hm, z_hm], dim=-1)\n",
    "    v_vp = xyz_vp - xyz_me\n",
    "    v_vm = xyz_vm - xyz_me\n",
    "    v_hp = xyz_hp - xyz_me\n",
    "    v_hm = xyz_hm - xyz_me\n",
    "    norm1 = torch.cross(v_vp, v_hp, dim=-1)\n",
    "    norm2 = torch.cross(v_hp, v_vm, dim=-1)\n",
    "    norm3 = torch.cross(v_vm, v_hm, dim=-1)\n",
    "    norm4 = torch.cross(v_hm, v_vp, dim=-1)\n",
    "    norm1 /= torch.norm(norm1, dim=-1)[:, :, None]\n",
    "    norm2 /= torch.norm(norm2, dim=-1)[:, :, None]\n",
    "    norm3 /= torch.norm(norm3, dim=-1)[:, :, None]\n",
    "    norm4 /= torch.norm(norm4, dim=-1)[:, :, None]\n",
    "    comparison12 = torch.isclose(norm1, norm2, atol=threshold, rtol=0)\n",
    "    comparison13 = torch.isclose(norm1, norm3, atol=threshold, rtol=0)\n",
    "    comparison14 = torch.isclose(norm1, norm4, atol=threshold, rtol=0)\n",
    "    all_same = comparison12 & comparison13 & comparison14\n",
    "    is_knot = torch.logical_not(torch.all(all_same, dim=-1))\n",
    "    knots = torch.stack(torch.where(is_knot), dim=-1) + 1\n",
    "    return knots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7170a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use double for more accurate knot finding\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set number of surfaces\n",
    "n_surface = 10\n",
    "\n",
    "# hidden sizes to consider\n",
    "n_hidden = [5, 20]\n",
    "\n",
    "# input\n",
    "n_grid = 1000\n",
    "x_in = torch.linspace(0, 1, n_grid)\n",
    "x_in = torch.stack(torch.meshgrid(x_in, x_in, indexing=\"ij\"), dim=-1).reshape([-1, 2])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 * len(n_hidden), figsize=(10, 3), dpi=200)\n",
    "plt.subplots_adjust(wspace=.15)\n",
    "for i_n, n in enumerate(n_hidden):\n",
    "    n_knots_def = 0\n",
    "    n_knots_kgi = 0\n",
    "    for i in range(n_surface):\n",
    "        # default model\n",
    "        model_def = MLP(2, hidden_size=n)\n",
    "        model_def.eval()\n",
    "\n",
    "        # KGI model\n",
    "        model_kgi = copy.deepcopy(model_def)\n",
    "        apply_kgi_to_model(model_kgi, knot_low=0.2, knot_high=0.8,\n",
    "                           perturb_factor=0.2, kgi_by_bias=False)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            y_def = model_def(x_in).reshape([n_grid, n_grid])\n",
    "            y_kgi = model_kgi(x_in).reshape([n_grid, n_grid])\n",
    "\n",
    "        # find knots\n",
    "        knots_def = find_knots(y_def, 1 / n_grid)\n",
    "        knots_kgi = find_knots(y_kgi, 1 / n_grid)\n",
    "        n_knots_def += len(knots_def)\n",
    "        n_knots_kgi += len(knots_kgi)\n",
    "\n",
    "        # plot\n",
    "        if i == 6:\n",
    "            axes[i_n + 0].imshow(y_def, cmap='RdYlBu')\n",
    "            axes[i_n + 0].scatter(knots_def[:, 1], knots_def[:, 0], marker=\".\",\n",
    "                                  s=1, c='k', lw=0, rasterized=True)\n",
    "            axes[i_n + 2].imshow(y_kgi, cmap='RdYlBu')\n",
    "            axes[i_n + 2].scatter(knots_kgi[:, 1], knots_kgi[:, 0], marker=\".\",\n",
    "                                  s=1, c='k', lw=0, rasterized=True)\n",
    "\n",
    "    # title\n",
    "    axes[i_n + 0].text(x=0.5, y=-0.1,\n",
    "                       s=\"No KGI, $H=%d$\" % (n,),\n",
    "                       fontsize=12, ha='center',\n",
    "                       transform=axes[i_n + 0].transAxes)\n",
    "    axes[i_n + 0].text(x=0.5, y=1.03,\n",
    "                       s=\"$N_\\\\mathrm{knot}=%d$\" % (round(n_knots_def / n_surface),),\n",
    "                       fontsize=12, ha='center', va='bottom',\n",
    "                       transform=axes[i_n + 0].transAxes)\n",
    "    axes[i_n + 2].text(x=0.5, y=-0.1,\n",
    "                       s=\"KGI, $H=%d$\" % (n,),\n",
    "                       fontsize=12, ha='center',\n",
    "                       transform=axes[i_n + 2].transAxes)\n",
    "    axes[i_n + 2].text(x=0.5, y=1.03,\n",
    "                       s=\"$N_\\\\mathrm{knot}=%d$\" % (round(n_knots_kgi / n_surface),),\n",
    "                       fontsize=12, ha='center', va='bottom',\n",
    "                       transform=axes[i_n + 2].transAxes)\n",
    "\n",
    "# setup\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.savefig(\"figs/surface_knots.pdf\", bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ae40e",
   "metadata": {},
   "source": [
    "# Surface fitting\n",
    "\n",
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "def gaussian(x, mu, sigma, height):\n",
    "    \"\"\" Gaussian shape \"\"\"\n",
    "    mu = torch.tensor(mu)[None, :]\n",
    "    exponent = -((x - mu).norm(dim=-1) ** 2) / (2 * sigma ** 2)\n",
    "    return height * torch.exp(exponent)\n",
    "\n",
    "\n",
    "# ground truth\n",
    "n_grid = 200  # using smaller size for training\n",
    "x_in = torch.linspace(0, 1, n_grid)\n",
    "x_in = torch.stack(torch.meshgrid(x_in, x_in, indexing=\"ij\"), dim=-1).reshape([-1, 2])\n",
    "y_true = torch.zeros_like(x_in[:, 0])\n",
    "for ix, xc in enumerate([0.1, 0.3, 0.5, 0.7, 0.9]):\n",
    "    for iy, yc in enumerate([0.1, 0.3, 0.5, 0.7, 0.9]):\n",
    "        y_true += gaussian(x_in, [xc, yc], 0.01, (ix + iy) % 2 - .5)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10 / 1.6 * 0.5, 3 / 1.6), dpi=200)\n",
    "ax2 = fig.gca()\n",
    "ax2.set_xticks([0, .5, 1.])\n",
    "ax2.set_yticks([0, .5, 1.])\n",
    "cax = ax2.imshow(y_true.reshape([n_grid, n_grid]), extent=[0, 1, 0, 1],\n",
    "                 aspect=1, cmap=\"seismic\")\n",
    "cbar = fig.colorbar(cax, ax=ax2, orientation='vertical', pad=0.1)\n",
    "plt.savefig(\"figs/surface_target.pdf\", bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb8b6f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kgi, hidden_size, num_hidden_layers, seed, activation=torch.relu,\n",
    "          num_epochs=10000, log_loss_every=20, device=\"cpu\", pbar=True):\n",
    "    \"\"\" Train a model \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    model = MLP(2, hidden_size=hidden_size, num_hidden_layers=num_hidden_layers,\n",
    "                activation=activation)\n",
    "    if kgi:\n",
    "        if activation is torch.nn.functional.tanh:\n",
    "            apply_kgi_to_model(model,\n",
    "                               knot_low=[0.2] + [-0.8] * (num_hidden_layers + 1),\n",
    "                               knot_high=[0.8] * (num_hidden_layers + 2),\n",
    "                               perturb_factor=0.2, kgi_by_bias=False)\n",
    "        elif activation is torch.nn.functional.gelu:\n",
    "            apply_kgi_to_model(model, knot_low=0.0, knot_high=1.0,\n",
    "                               perturb_factor=0.2, kgi_by_bias=False)\n",
    "        else:\n",
    "            apply_kgi_to_model(model, knot_low=0.2, knot_high=0.8,\n",
    "                               perturb_factor=0.2, kgi_by_bias=False)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # training loop\n",
    "    loss_hist = []\n",
    "    x_in_ = x_in.to(device)\n",
    "    y_true_ = y_true.unsqueeze(1).to(device)\n",
    "    for epoch in trange(num_epochs, disable=not pbar):\n",
    "        y_pred = model(x_in_)\n",
    "        loss = criterion(y_pred, y_true_)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % log_loss_every == 0:\n",
    "            loss_hist.append(loss.item())\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all models\n",
    "reproduce_paper = False\n",
    "if reproduce_paper:\n",
    "    \"\"\" THIS WILL TAKE A LONG TIME TO TRAIN THE 10x10x4x2 MODELS \"\"\"\n",
    "    seeds = list(range(10))\n",
    "    nn_sizes = [(25, 1), (50, 1), (100, 1), (200, 1), (400, 1), (600, 1),\n",
    "                (25, 2), (25, 3), (25, 4), (25, 5)]\n",
    "    activations = [torch.nn.functional.relu, torch.nn.functional.leaky_relu,\n",
    "                   torch.nn.functional.gelu, torch.nn.functional.tanh]\n",
    "    epochs = 30000\n",
    "    device_ = \"cuda\"\n",
    "    out_dir = Path(\"results/surface_paper\")\n",
    "    show_pbar = False\n",
    "else:\n",
    "    seeds = [0]\n",
    "    nn_sizes = [(200, 1)]\n",
    "    activations = [torch.nn.functional.relu]\n",
    "    epochs = 3000\n",
    "    device_ = \"cpu\"\n",
    "    out_dir = Path(\"results/surface_test\")\n",
    "    show_pbar = True\n",
    "\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "for seed_ in seeds:\n",
    "    for act_ in activations:\n",
    "        for hidden_size_, num_hidden_layers_ in nn_sizes:\n",
    "            for kgi_ in [False, True]:\n",
    "                act_name_ = \"gelu\" if act_ is torch.nn.functional.gelu else str(act_).split(\" \")[1]\n",
    "                name_ = f\"{act_name_}_{seed_}_{hidden_size_}_{num_hidden_layers_}_{kgi_}\"\n",
    "                if not (out_dir / name_).exists():\n",
    "                    t0 = time()\n",
    "                    _, hist_ = train(kgi_, hidden_size_, num_hidden_layers_, seed_, act_, device=device_,\n",
    "                                     num_epochs=epochs, pbar=show_pbar)\n",
    "                    np.savetxt(out_dir / name_, hist_)\n",
    "                    print(f\"{name_} trained in {(time() - t0) / 60:.1f} min, loss={hist_[-1]:.2e}\")\n",
    "                else:\n",
    "                    print(f\"{name_} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2556fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick comparison\n",
    "hist_def = np.loadtxt(out_dir / \"relu_0_200_1_False\")\n",
    "hist_kgi = np.loadtxt(out_dir / \"relu_0_200_1_True\")\n",
    "plt.plot(hist_def)\n",
    "plt.plot(hist_kgi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc976c",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The following cells works only when `reproduce_paper` was set `True` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = list(range(10))\n",
    "nn_sizes = [(25, 1), (50, 1), (100, 1), (200, 1), (400, 1), (600, 1),\n",
    "            (25, 2), (25, 3), (25, 4), (25, 5)]\n",
    "epochs = 30000\n",
    "out_dir = Path(\"results/surface_paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss statistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe49044eda6dca6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ecc800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(act_name, kgi):\n",
    "    min_loss_mean = []\n",
    "    min_loss_std = []\n",
    "    for hidden_size, num_hidden_layers in nn_sizes:\n",
    "        hists = []\n",
    "        for seed in seeds:\n",
    "            name = f\"{act_name}_{seed}_{hidden_size}_{num_hidden_layers}_{kgi}\"\n",
    "            hist = np.loadtxt(out_dir / name)\n",
    "            hists.append(hist)\n",
    "        hists = np.array(hists) * 10000\n",
    "        min_loss = np.mean(hists[:, -5:], axis=1)\n",
    "        min_loss = np.sort(min_loss)\n",
    "        min_loss_mean.append(min_loss[:-4].mean())\n",
    "        min_loss_std.append(min_loss[:-4].std())\n",
    "    return np.array(min_loss_mean), np.array(min_loss_std)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(9, 2.5), dpi=200, sharex=True)\n",
    "plt.subplots_adjust(wspace=.22)\n",
    "for i, (act, act_str) in enumerate(zip([\"relu\", \"leaky_relu\", \"gelu\", \"tanh\"],\n",
    "                                       [\"ReLU\", \"LeakyReLU\", \"GELU\", \"Tanh\"])):\n",
    "    min_mean_def, min_std_def = get_metrics(act, False)\n",
    "    min_mean_kgi, min_std_kgi = get_metrics(act, True)\n",
    "    x_fake = range(len(min_mean_def))\n",
    "    axes[i].errorbar(x_fake, min_mean_def, min_std_def, fmt=\"o\",\n",
    "                     markersize=3, capsize=2, capthick=1, elinewidth=1, label=\"No KGI\", c=\"b\")\n",
    "    axes[i].errorbar(x_fake, min_mean_kgi, min_std_def, fmt=\"s\",\n",
    "                     markersize=3, capsize=2, capthick=1, elinewidth=1, label=\"KGI\", c=\"r\")\n",
    "    axes[i].set_xticks(x_fake, [\"25,1\", \"50,1\", \"100,1\", \"200,1\", \"400,1\", \"600,1\",\n",
    "                                \"25,2\", \"25,3\", \"25,4\", \"25,5\"])\n",
    "    for label in axes[i].get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "    axes[i].set_xlabel(\"Width $W$, Depth $D$\")\n",
    "    axes[i].text(.5, 1.03, f\"$\\sigma$: {act_str}\", va=\"bottom\", ha=\"center\",\n",
    "                 transform=axes[i].transAxes)\n",
    "\n",
    "axes[0].set_ylabel(\"MSE ($10^{-4}$)\")\n",
    "axes[2].legend()\n",
    "plt.savefig(\"figs/surface_loss_WD.pdf\", bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66de25721f070bc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_ave(data, window_size):\n",
    "    \"\"\" Moving average to smooth the loss history a little bit \"\"\"\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "\n",
    "_, axes = plt.subplots(2, 5, sharex=True, sharey=True, figsize=(9, 3), dpi=200)\n",
    "plt.subplots_adjust(wspace=.15)\n",
    "for i, (hidden_size_, num_hidden_layers_) in enumerate(nn_sizes):\n",
    "    seed_ = 0\n",
    "    hist_def = np.loadtxt(out_dir / f\"relu_{seed_}_{hidden_size_}_{num_hidden_layers_}_{False}\")\n",
    "    hist_kgi = np.loadtxt(out_dir / f\"relu_{seed_}_{hidden_size_}_{num_hidden_layers_}_{True}\")\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    p = 5\n",
    "    ax.plot(np.arange(epochs // 20 - p // 2 - 2), moving_ave(hist_def, p) * 10000, label=\"No KGI\", lw=1, c='b')\n",
    "    ax.plot(np.arange(epochs // 20 - p // 2 - 2), moving_ave(hist_kgi, p) * 10000, label=\"KGI\", lw=1, c='r')\n",
    "    ax.text(.5, .92, f\"$W={hidden_size_}$\", va=\"top\", ha=\"left\", transform=ax.transAxes)\n",
    "    ax.text(.5, .78, f\"$D={num_hidden_layers_}$\", va=\"top\", ha=\"left\", transform=ax.transAxes)\n",
    "\n",
    "axes[0, 0].set_ylim(0, 40)\n",
    "for i in range(2):\n",
    "    axes[i, 0].set_ylabel(\"MSE ($10^{-4}$)\")\n",
    "for i in range(5):\n",
    "    axes[1, i].set_xlabel(\"Epoch ($10^3$)\")\n",
    "    axes[1, i].set_xticks([0, 10000 // 20, 20000 // 20, 30000 // 20], [0, 10, 20, 30])\n",
    "axes[0, 0].legend(ncol=2, loc=[0.03, 0.04], handlelength=.8, columnspacing=.5, handletextpad=.4)\n",
    "plt.savefig(\"figs/surface_loss_epoch.pdf\", bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb3f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
