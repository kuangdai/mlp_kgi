{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from kgi import apply_kgi_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b436357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to enable LaTeX rendering\n",
    "# change to False if get an error during plotting (latex not installed)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_init_to_he_uniform(model):\n",
    "    \"\"\" Change PyTorch's default initialization to He uniform \"\"\"\n",
    "    # PyTorch default uses 1/sqrt(m) as the bound\n",
    "    # He uniform uses sqrt(3) / sqrt(m) as the bound\n",
    "    sqrt3 = np.sqrt(3)\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            layer.weight.data *= sqrt3\n",
    "            layer.bias.data *= sqrt3\n",
    "\n",
    "\n",
    "class MLP1d(nn.Module):\n",
    "    def __init__(self, hidden_size, activation=torch.relu):\n",
    "        super(MLP1d, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.act = activation\n",
    "        default_init_to_he_uniform(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dec2dc",
   "metadata": {},
   "source": [
    "# Visualize initialized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c21ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_knots(y, dx, threshold=1e-11):\n",
    "    \"\"\" Find indices of knots in a curve \"\"\"\n",
    "    if len(y) < 2:\n",
    "        return []\n",
    "    # calculate the slopes between consecutive points\n",
    "    slopes = (y[1:] - y[:-1]) / dx\n",
    "    # find where the slopes change\n",
    "    slope_changes = torch.abs(torch.diff(slopes))\n",
    "    knots = torch.where(slope_changes > threshold)[0] + 1  # noqa\n",
    "    return knots.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7170a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use double for more accurate knot finding\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set number of curves\n",
    "n_curve = 10\n",
    "\n",
    "# hidden sizes to consider\n",
    "n_hidden = [5, 20]\n",
    "\n",
    "# input\n",
    "x_in = torch.linspace(0, 1, 1000).unsqueeze(1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 * len(n_hidden), figsize=(10, 3), dpi=200)\n",
    "plt.subplots_adjust(wspace=.15)\n",
    "for i_n, n in enumerate(n_hidden):\n",
    "    n_knots_def = 0\n",
    "    n_knots_kgi = 0\n",
    "    for i in range(n_curve):\n",
    "        # default model\n",
    "        model_def = MLP1d(n)\n",
    "        model_def.eval()\n",
    "\n",
    "        # KGI model\n",
    "        model_kgi = copy.deepcopy(model_def)\n",
    "        apply_kgi_to_model(model_kgi, knot_low=0.2, knot_high=0.8,\n",
    "                           perturb_factor=0.2, kgi_by_bias=False)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            y_def = model_def(x_in).squeeze(1)\n",
    "            y_kgi = model_kgi(x_in).squeeze(1)\n",
    "\n",
    "        # find knots\n",
    "        knots_def = find_knots(y_def, x_in[1] - x_in[0])\n",
    "        knots_kgi = find_knots(y_kgi, x_in[1] - x_in[0])\n",
    "        n_knots_def += len(knots_def)\n",
    "        n_knots_kgi += len(knots_kgi)\n",
    "\n",
    "        # plot\n",
    "        axes[i_n + 0].plot(x_in, y_def, lw=1)\n",
    "        axes[i_n + 0].scatter(x_in[knots_def], y_def[knots_def], marker=\"|\", s=80, lw=.5)\n",
    "        axes[i_n + 2].plot(x_in, y_kgi, lw=1)\n",
    "        axes[i_n + 2].scatter(x_in[knots_kgi], y_kgi[knots_kgi], marker=\"|\", s=80, lw=.5)\n",
    "\n",
    "    # title\n",
    "    axes[i_n + 0].text(x=0.5, y=-0.1,\n",
    "                       s=\"(%s) No KGI, $H=%d$\" % (chr(ord('a') + i_n), n),\n",
    "                       fontsize=12, ha='center',\n",
    "                       transform=axes[i_n + 0].transAxes)\n",
    "    axes[i_n + 0].text(x=0.5, y=1.03,\n",
    "                       s=\"$N_\\\\mathrm{knot}=%d$\" % (round(n_knots_def / n_curve),),\n",
    "                       fontsize=12, ha='center', va='bottom',\n",
    "                       transform=axes[i_n + 0].transAxes)\n",
    "    axes[i_n + 2].text(x=0.5, y=-0.1,\n",
    "                       s=\"(%s) KGI, $H=%d$\" % (chr(ord('c') + i_n), n,),\n",
    "                       fontsize=12, ha='center',\n",
    "                       transform=axes[i_n + 2].transAxes)\n",
    "    axes[i_n + 2].text(x=0.5, y=1.03,\n",
    "                       s=\"$N_\\\\mathrm{knot}=%d$\" % (round(n_knots_kgi / n_curve),),\n",
    "                       fontsize=12, ha='center', va='bottom',\n",
    "                       transform=axes[i_n + 2].transAxes)\n",
    "\n",
    "# setup\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(0, 1)\n",
    "plt.savefig(\"figs/curve_knots.pdf\", bbox_inches='tight', pad_inches=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c74326",
   "metadata": {},
   "source": [
    "# Curve fitting\n",
    "\n",
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "def gaussian(x, mu, sigma, height):\n",
    "    \"\"\" Gaussian shape \"\"\"\n",
    "    exponent = -((x - mu) ** 2) / (2 * sigma ** 2)\n",
    "    return height * torch.exp(exponent)\n",
    "\n",
    "\n",
    "# ground truth\n",
    "x_in = torch.linspace(0, 1, 1000)\n",
    "y_true = torch.zeros_like(x_in)\n",
    "y_true += gaussian(x_in, 0.15, 0.005, 1)\n",
    "y_true -= gaussian(x_in, 0.34, 0.003, .8)\n",
    "y_true -= gaussian(x_in, 0.41, 0.003, .6)\n",
    "y_true += gaussian(x_in, 0.62, 0.004, 1.2)\n",
    "y_true += gaussian(x_in, 0.8, 0.002, 0.8)\n",
    "plt.figure(figsize=(10 / 3, 8 / 3), dpi=200)\n",
    "plt.xlim(0, 1)\n",
    "plt.plot(x_in, y_true, lw=1)\n",
    "\n",
    "# dataset\n",
    "dataset = TensorDataset(x_in.unsqueeze(1), y_true.unsqueeze(1))\n",
    "dataloader = DataLoader(dataset, batch_size=1000, shuffle=True)  # no need to batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441aa8e3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f15f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kgi, hidden_size, seed, activation=torch.relu,\n",
    "          num_epochs=30000, log_loss_every=100, device=\"cpu\", pbar=True):\n",
    "    \"\"\" Train a model \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    model = MLP1d(hidden_size, activation)\n",
    "    if kgi:\n",
    "        apply_kgi_to_model(model, knot_low=0.2, knot_high=0.8,\n",
    "                           perturb_factor=0.2, kgi_by_bias=False)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # training loop\n",
    "    loss_hist = []\n",
    "    for epoch in trange(num_epochs, disable=not pbar):\n",
    "        running_loss = 0.0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "        if (epoch + 1) % log_loss_every == 0:\n",
    "            loss_hist.append(running_loss / len(dataset))\n",
    "    return model, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all models\n",
    "reproduce_paper = False\n",
    "if reproduce_paper:\n",
    "    seeds = [0, 1, 2, 3, 4]\n",
    "    hidden_sizes = [25, 50, 100, 200, 400, 600, 800, 1000, 1200]\n",
    "    activations = [torch.nn.functional.relu, torch.nn.functional.leaky_relu,\n",
    "                   torch.nn.functional.glu, torch.nn.functional.tanh]\n",
    "    epochs = 30000\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    seeds = [0]\n",
    "    hidden_sizes = [200]\n",
    "    activations = [torch.nn.functional.relu]\n",
    "    epochs = 10000\n",
    "    device = \"cpu\"\n",
    "\n",
    "out_dir = Path(\"results/curve\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "for act_ in activations:\n",
    "    for seed_ in seeds:\n",
    "        for hidden_size_ in hidden_sizes:\n",
    "            for kgi_ in [False, True]:\n",
    "                act_name = str(act_).split(\" \")[1]\n",
    "                name = f\"{act_name}_{seed_}_{hidden_size_}_{kgi_}\"\n",
    "                t0 = time()\n",
    "                _, hist = train(kgi_, hidden_size_, seed_, act_, device=device,\n",
    "                                num_epochs=epochs, pbar=False)\n",
    "                np.savetxt(out_dir / name, hist)\n",
    "                print(f\"{name} trained in {(time() - t0) / 60:.1f} min, loss={hist[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388db2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick comparison\n",
    "hist_def = np.loadtxt(out_dir / \"relu_0_200_False\")\n",
    "hist_kgi = np.loadtxt(out_dir / \"relu_0_200_True\")\n",
    "plt.plot(hist_def)\n",
    "plt.plot(hist_kgi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c2ece",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "The following cells works only when `reproduce_paper` was set `True` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b9168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
