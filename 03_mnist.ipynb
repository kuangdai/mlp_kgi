{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0322c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import trange\n",
    "\n",
    "from kgi import apply_kgi_to_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b436357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to enable LaTeX rendering\n",
    "# change to `False` if you get an error during plotting (latex not installed)\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dc013-ae78-440b-a258-d8da142413d0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3431b07-4528-40ae-8c3e-204e4fbac9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, labels, batch_size):\n",
    "    \"\"\" manually batchify data \"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], labels[i:i + batch_size]\n",
    "\n",
    "\n",
    "def load_dataset_to_gpu(batch_size=256, device=\"cuda\"):\n",
    "    \"\"\" load dataset in CPU, process, and move to GPU \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    train_dataset = torchvision.datasets.FashionMNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.FashionMNIST(root='./datasets', train=False, download=True, transform=transform)\n",
    "    train_data = torch.stack([train_dataset[i][0] for i in range(len(train_dataset))]).to(device)\n",
    "    train_labels = torch.tensor([train_dataset[i][1] for i in range(len(train_dataset))], dtype=torch.long).to(device)\n",
    "    test_data = torch.stack([test_dataset[i][0] for i in range(len(test_dataset))]).to(device)\n",
    "    test_labels = torch.tensor([test_dataset[i][1] for i in range(len(test_dataset))], dtype=torch.long).to(device)\n",
    "    train_batches = list(batchify(train_data, train_labels, batch_size))\n",
    "    test_batches = list(batchify(test_data, test_labels, batch_size))\n",
    "    return train_batches, test_batches\n",
    "\n",
    "\n",
    "device_ = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_set, test_set = load_dataset_to_gpu(256, device=device_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c74326",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6a052-6a2d-4609-81c2-9f872b03b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)  # Input size is 28x28, output size is 256\n",
    "        self.fc2 = nn.Linear(256, 128)  # Hidden layer with 128 units\n",
    "        self.fc3 = nn.Linear(128, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
    "        x = torch.relu(self.fc1(x))  # First hidden layer\n",
    "        x = torch.relu(self.fc2(x))  # Second hidden layer\n",
    "        x = self.fc3(x)  # Output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kgi, seed=0, num_epochs=10000, save_every=20, progress_bar=True):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # model\n",
    "    model = FashionMNISTMLP().to(device_)\n",
    "    if kgi:\n",
    "        apply_kgi_to_model(model, knot_low=[-0.8, 0., 0.], knot_high=0.8,\n",
    "                           perturb_factor=0.2, kgi_by_bias=False)\n",
    "    model.train()\n",
    "\n",
    "    # loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # train loop\n",
    "    loss_history = []\n",
    "    loop = trange(num_epochs, desc=\"Training Epochs\", disable=not progress_bar)\n",
    "    for epoch in loop:\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_set:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            epoch_loss = running_loss / len(train_set)\n",
    "            loss_history.append(epoch_loss)\n",
    "            loop.set_postfix({\"Epoch Loss\": f\"{epoch_loss:.2e}\"})\n",
    "\n",
    "    # evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_set:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  # noqa\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    if progress_bar:\n",
    "        print(f'Final Accuracy on test set: {accuracy:.2f}%')\n",
    "    return loss_history, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train all models\n",
    "seeds = list(range(10))  # use `seeds = [0]` for fast test\n",
    "epochs = 10000  # use a smaller one for fast test\n",
    "out_dir = Path(\"results/mnist_paper\")\n",
    "\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "for seed_ in seeds:\n",
    "    for kgi_ in [False, True]:\n",
    "        name_ = f\"{seed_}_{kgi_}\"\n",
    "        if not (out_dir / name_).exists():\n",
    "            t0 = time()\n",
    "            hist_, acc = train(kgi_, seed_, epochs, progress_bar=False)\n",
    "            np.savetxt(out_dir / name_, hist_, header=f\"{acc}\")\n",
    "            print(f\"{name_} trained in {(time() - t0) / 60:.1f} min, loss={hist_[-1]:.2e}, acc={acc:.2f}%\")\n",
    "        else:\n",
    "            print(f\"{name_} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39740eca-af8c-4aeb-aa7b-e2a5d1d3184c",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcaaa2f-eedc-4fc4-bd99-bbbec8980c45",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a5c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(kgi):\n",
    "    losses = []\n",
    "    speeds = []\n",
    "    accs = []\n",
    "    for seed in seeds:\n",
    "        # read history\n",
    "        name = f\"{seed}_{kgi}\"\n",
    "        hist = np.loadtxt(out_dir / name)\n",
    "        # use average of last 100 epochs (5 * 20) as final loss\n",
    "        final_loss = hist[-5:].mean()\n",
    "        losses.append(final_loss)\n",
    "        # AUC for convergence speed\n",
    "        speeds.append(1. / (hist.sum() - final_loss))\n",
    "        # read relative error\n",
    "        with open(out_dir / name) as fs:\n",
    "            acc_str = fs.readline()\n",
    "        accs.append(float(acc_str[1:]))\n",
    "    losses = np.array(losses)\n",
    "    speeds = np.array(speeds)\n",
    "    accs = np.array(accs)\n",
    "    print(\"KGI\" if kgi else \"No KGI\")\n",
    "    # print in latex format\n",
    "    print(f\"Loss: {losses.mean():.1e} \\pm {losses.std():.1e}\")\n",
    "    print(f\"Speed: {speeds.mean():.1e} \\pm {speeds.std():.1e}\")\n",
    "    print(f\"Accuracy: {accs.mean():.1f}\\% \\pm {accs.std():.1f}\\%\")\n",
    "\n",
    "\n",
    "print_metrics(False)\n",
    "print_metrics(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ec181-7be4-46f2-9522-a13b1be479fb",
   "metadata": {},
   "source": [
    "### Loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69588ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_ave(series, window_size):\n",
    "    \"\"\" Moving average to smooth the loss history a little bit \"\"\"\n",
    "    return np.convolve(series, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5 / 1.5, 4 / 1.5), dpi=200)\n",
    "seed_ = 0\n",
    "hist_def = np.loadtxt(out_dir / f\"{seed_}_{False}\")\n",
    "hist_kgi = np.loadtxt(out_dir / f\"{seed_}_{True}\")\n",
    "ax.plot(np.arange(0, epochs // 20), moving_ave(hist_def, 1), label=\"No KGI\", lw=1, c='b')\n",
    "ax.plot(np.arange(0, epochs // 20), moving_ave(hist_kgi, 1), label=\"KGI\", lw=1, c='r')\n",
    "ax.set_xticks([0, 2000 // 20, 4000 // 20, 6000 // 20, 8000 // 20, 10000 // 20],\n",
    "              [0, 2, 4, 6, 8, 10])\n",
    "ax.set_ylabel(\"Cross entropy loss\")\n",
    "ax.set_xlabel(\"Epoch ($10^3$)\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(ncol=2, handlelength=.8, columnspacing=.5, handletextpad=.4)\n",
    "# not saving figure here because we will plot all histories together \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33021307-c40b-43a6-99b4-8ed5e2693392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
